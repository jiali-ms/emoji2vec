# emoji2vec
[<img src="https://github.com/jiali-ms/emoji2vec/blob/master/logo.png">](http://nlpfun.com/emoji)

A demo project to play word embedding and emoji with twitter data. Let's see how we can make a smarter emoji predictor.  
  
Believe it or not, we have 1800+ emoji from standard Unicode. How to find one from them? Traditionally, we search key words of emoji description. We will use word embedding to find the best match with a context. The results reflects real users habit from social media. Now you are guided with most knowledgeable emoji master :)

Check the site http://nlpfun.com/emoji for a preview of what we can do next with the model!  

##   Data 
The zip file in the [data](https://github.com/jiali-ms/emoji2vec/tree/master/data) folder is a 1M sentences with emoji from Twitter about 2017-Jan. It is randomly selected set from a much bigger corpus. Unzip the corpus.txt directly into the data folder for training. 
## Training
Run [train.py](https://github.com/jiali-ms/emoji2vec/blob/master/train.py) file. Don't forget to set correct parameters like vector size, windows size, etc. It will dump a model and a raw text file for the embedding.   

Install the gensim first with python 3.5  
> pip install -r requirements.txt 

## Results
Let's run the [model.py](https://github.com/jiali-ms/emoji2vec/blob/master/model.py) and see the console output. If you ever played word2vec before, you know the answer for the famous play **'King' - 'man' + 'woman' = ?** . Let check the results with our simple twitter data.
> query + [**'king'**, **'woman'**] - [**'man'**]  
[(**'queen'**, 0.73), ('crown', 0.7), ('goddess', 0.7), ('princess', 0.7), ('actress', 0.69)]

>query + [**'china'**, **'tokyo'**] - [**'beijing'**]  
[(**'japan'**, 0.73), ('theatre', 0.69), ('europe', 0.68), ('nyc', 0.67), ('nuttiness', 0.67)]

>query + [**'dog'**, **'cats'**] - [**'dogs'**]  
[(**'cat'**, 0.92), ('kitten', 0.75), ('puppy', 0.73), ('coworker', 0.71), ('mom', 0.69)]

Finally, let's see how well we can find emoji by key words!
> query + [**'cat'**] - []  
[('ğŸ±', 0.66), ('ğŸ¶', 0.5), ('ğŸˆ', 0.47), ('ğŸ°', 0.42), ('ğŸ•', 0.4), ('ğŸ­', 0.39)]

>query + [**'happy'**, **'new'**, **'year'**] - []  
[('ğŸ‰', 0.56), ('ğŸ„', 0.38), ('ğŸ†', 0.38), ('ğŸ¾', 0.37), ('ğŸ', 0.36)]

>query + [**'king'**, **'woman'**] - [**'man'**]  
[('ğŸ‘‘', 0.63), ('ğŸ‘¸', 0.59), ('ğŸ¦', 0.56)]

>query + ['ğŸ’—'] - []  
[('ğŸ’“', 0.94), ('ğŸ’œ', 0.92), ('ğŸ’–', 0.92), ('ğŸ’˜', 0.91), ('ğŸ’', 0.91), ('ğŸ’•', 0.91), ('ğŸ’›', 0.81), ('ğŸ’™', 0.81), ('ğŸ’Ÿ', 0.79), ('ğŸ’', 0.72)]

To better understanding what's going on inside, here are some recommendations:  
https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
http://web.stanford.edu/class/cs224n/assignment1/index.html 

